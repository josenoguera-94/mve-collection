# Databricks Local con Docker (Spark + Delta + Unity Catalog Sim)

Entorno Databricks local de alta fidelidad usando Docker. Emula el Runtime de Databricks 14.3/15.x LTS (Apache Spark 3.5.2 + Delta Lake 3.2.0) con Cloud Storage local (MinIO) y Metastore persistente (PostgreSQL para simulaci√≥n de Unity Catalog).

## Estructura del Proyecto

```
databricks-docker/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ databricks_shim/   # Capa de abstracci√≥n (Local vs Cloud)
‚îÇ   ‚îî‚îÄ‚îÄ jobs/              # L√≥gica ETL
‚îú‚îÄ‚îÄ Dockerfile             # Imagen personalizada tipo DBR
‚îú‚îÄ‚îÄ docker-compose.yml     # Orquestaci√≥n (Spark, MinIO, Postgres)
‚îú‚îÄ‚îÄ .env                   # Configuraci√≥n de entorno
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias Python
‚îî‚îÄ‚îÄ README.md
```

## Requisitos Previos

- Docker y Docker Compose instalados

## Opci√≥n 1: Usando Docker Compose

### Paso 1: Construir e Iniciar

Esto construir√° la imagen personalizada de Spark e iniciar√° MinIO y Postgres.

```bash
docker compose up -d --build
```

### Paso 2: Crear el Bucket de Almacenamiento

1. Abre la consola de MinIO: http://localhost:9001
2. Inicia sesi√≥n: Usuario: `minioadmin` / Password: `minioadmin`
3. Ve a **Buckets** -> **Create Bucket**.
4. Crea un bucket llamado: `demo-bucket` (**CR√çTICO**: El ETL fallar√° si este bucket no existe).

### Paso 3: Ejecutar el Job ETL

Ejecuta el ETL de ejemplo que:
1. Genera datos y los escribe en Bronze (MinIO)
2. Transforma y escribe en Silver (Tabla Delta en Metastore)
3. Registra la tabla en el Hive Metastore persistente

```bash
docker compose exec spark python3 src/jobs/etl_sample.py
```

Deber√≠as ver:
```text
üöÄ Starting ETL Job...
üíæ Writing Bronze Layer...
üíæ Writing Silver Layer...
‚úÖ ETL Job Completed Successfully!
üìä Verification Query:
+---+---------+-----+----------+...
| id|     name|price|      date|...
+---+---------+-----+----------+...
```

### Paso 3: Verificar Persistencia

1. **MinIO Console**: http://localhost:9001 (User/Pass: `minioadmin`)
   - Revisa `demo-bucket` para ver carpetas `bronze/` y `silver/`.
2. **Salida Spark**: La consulta `SELECT` confirma que el Metastore funciona.

## Componentes del Proyecto

### Imagen Personalizada (`Dockerfile`)

Construimos una imagen FROM `databricksruntime/python:latest` e instalamos manualmente:
- **OpenJDK 17**: Requerido por Spark 3.5+
- **Apache Spark 3.5.2**: Coincide con DBR 15.x LTS
- **Delta Lake 3.2.0**: Para transacciones ACID
- **Hadoop AWS**: Para soporte de sistema de archivos S3A

### Capa Shim (`src/databricks_shim/`)

Permite escribir c√≥digo portable:
- **`connect.py`**: Detecta `APP_ENV`. Si es `local`, inyecta configuraciones de MinIO, Delta y Postgres en la `SparkSession`.
- **`utils.py`**: Mockea `dbutils` (Secrets, Widgets) usando variables de entorno al ejecutar localmente.

### Infraestructura

- **MinIO**: Emula S3 / ADLS Gen2.
- **PostgreSQL**: Act√∫a como Hive Metastore persistente (simulando tablas Unity Catalog).

## Variables de Entorno

El archivo `.env` configura credenciales y endpoints:

```
AWS_ENDPOINT_URL=http://minio:9000
POSTGRES_HOST=postgres
APP_ENV=local
```

## Limpieza

```bash
docker compose down -v
```

## Licencia

Ejemplo m√≠nimo viable con fines educativos.
