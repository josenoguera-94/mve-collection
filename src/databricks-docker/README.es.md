# Databricks Local con Docker (Spark + Delta + Unity Catalog Sim)

Entorno Databricks local de alta fidelidad usando Docker. Emula el Runtime de Databricks 14.3/15.x LTS (Apache Spark 3.5.2 + Delta Lake 3.2.0) con Cloud Storage local (MinIO) y Metastore persistente (PostgreSQL para simulaciÃ³n de Unity Catalog).

## Estructura del Proyecto

```
databricks-docker/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ databricks_shim/   # Capa de abstracciÃ³n (Local vs Cloud)
â”‚   â””â”€â”€ jobs/              # LÃ³gica ETL
â”œâ”€â”€ Dockerfile             # Imagen personalizada tipo DBR
â”œâ”€â”€ docker-compose.yml     # OrquestaciÃ³n (Spark, MinIO, Postgres)
â”œâ”€â”€ .env                   # ConfiguraciÃ³n de entorno
â”œâ”€â”€ requirements.txt       # Dependencias Python
â””â”€â”€ README.md
```

## Requisitos Previos

- Docker y Docker Compose instalados

## OpciÃ³n 1: Usando Docker Compose

### Paso 1: Construir e Iniciar

Esto construirÃ¡ la imagen personalizada de Spark e iniciarÃ¡ MinIO y Postgres.

```bash
docker compose up -d --build
```

### Paso 2: Ejecutar el Job ETL

Ejecuta el ETL de ejemplo que:
1. Genera datos y los escribe en Bronze (MinIO)
2. Transforma y escribe en Silver (Tabla Delta en Metastore)
3. Registra la tabla en el Hive Metastore persistente

```bash
docker compose exec spark python src/jobs/etl_sample.py
```

DeberÃ­as ver:
```text
ðŸš€ Starting ETL Job...
ðŸ’¾ Writing Bronze Layer...
ðŸ’¾ Writing Silver Layer...
âœ… ETL Job Completed Successfully!
ðŸ“Š Verification Query:
+---+---------+-----+----------+...
| id|     name|price|      date|...
+---+---------+-----+----------+...
```

### Paso 3: Verificar Persistencia

1. **MinIO Console**: http://localhost:9001 (User/Pass: `minioadmin`)
   - Revisa `demo-bucket` para ver carpetas `bronze/` y `silver/`.
2. **Salida Spark**: La consulta `SELECT` confirma que el Metastore funciona.

## Componentes del Proyecto

### Imagen Personalizada (`Dockerfile`)

Construimos una imagen FROM `databricksruntime/python:latest` e instalamos manualmente:
- **OpenJDK 17**: Requerido por Spark 3.5+
- **Apache Spark 3.5.2**: Coincide con DBR 15.x LTS
- **Delta Lake 3.2.0**: Para transacciones ACID
- **Hadoop AWS**: Para soporte de sistema de archivos S3A

### Capa Shim (`src/databricks_shim/`)

Permite escribir cÃ³digo portable:
- **`connect.py`**: Detecta `APP_ENV`. Si es `local`, inyecta configuraciones de MinIO, Delta y Postgres en la `SparkSession`.
- **`utils.py`**: Mockea `dbutils` (Secrets, Widgets) usando variables de entorno al ejecutar localmente.

### Infraestructura

- **MinIO**: Emula S3 / ADLS Gen2.
- **PostgreSQL**: ActÃºa como Hive Metastore persistente (simulando tablas Unity Catalog).

## Variables de Entorno

El archivo `.env` configura credenciales y endpoints:

```
AWS_ENDPOINT_URL=http://minio:9000
POSTGRES_HOST=postgres
APP_ENV=local
```

## Limpieza

```bash
docker compose down -v
```

## Licencia

Ejemplo mÃ­nimo viable con fines educativos.
